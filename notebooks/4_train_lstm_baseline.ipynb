{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4890b20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-17T23:47:02.370067Z",
     "start_time": "2022-03-17T23:47:01.258722Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import argparse\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics.functional as tF\n",
    "import pytorch_lightning as pl\n",
    "import tokenizers\n",
    "import datasets\n",
    "\n",
    "\n",
    "DEBUG_RUN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e7dfc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-17T23:47:02.374151Z",
     "start_time": "2022-03-17T23:47:02.371270Z"
    }
   },
   "outputs": [],
   "source": [
    "class HFDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hfdf):\n",
    "        self.hfdf = hfdf\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.hfdf[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hfdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cae9a71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-17T23:47:02.393611Z",
     "start_time": "2022-03-17T23:47:02.375779Z"
    }
   },
   "outputs": [],
   "source": [
    "class LitSegmenterBaseline(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        tokenizer_uri: str,\n",
    "        dataset_uri: str,\n",
    "        batch_size: int,\n",
    "        num_layers: int = 1,\n",
    "        bidirectional: bool = True,\n",
    "        num_classes: int = 4,\n",
    "        pad_token: str = \"[PAD]\",\n",
    "    ):\n",
    "        super(LitSegmenterBaseline, self).__init__()\n",
    "\n",
    "        self.tokenizer = tokenizers.Tokenizer.from_file(tokenizer_uri)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.pad_id = self.tokenizer.get_vocab().get(pad_token, 0)\n",
    "\n",
    "        def fn_pad_sequences(batch):\n",
    "            X = [torch.tensor(x_i[\"input_ids\"], dtype=torch.int) for x_i in batch]\n",
    "            y = [torch.tensor(y_i[\"labels\"]) for y_i in batch]\n",
    "\n",
    "            X = nn.utils.rnn.pad_sequence(X, padding_value=self.pad_id, batch_first=True)\n",
    "            y = nn.utils.rnn.pad_sequence(y, padding_value=-100, batch_first=True)\n",
    "\n",
    "            return X, y\n",
    "\n",
    "        self.fn_pad_sequences = fn_pad_sequences\n",
    "\n",
    "        self.hfdf = datasets.load_from_disk(dataset_uri)\n",
    "\n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=self.tokenizer.get_vocab_size(),\n",
    "            embedding_dim=768,\n",
    "            padding_idx=self.pad_id,\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=768,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.0 if num_layers == 1 else 0.1,\n",
    "            bidirectional=bidirectional,\n",
    "            proj_size=0,\n",
    "        )\n",
    "\n",
    "        self.lin_out = nn.Linear(\n",
    "            (1 + int(bidirectional)) * hidden_size,\n",
    "            num_classes,\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = X\n",
    "\n",
    "        if isinstance(out, str):\n",
    "            out = self.tokenizer(out, return_tensors=\"pt\")\n",
    "            out = out[\"input_ids\"]\n",
    "\n",
    "        out = self.embeddings(out)\n",
    "        out, *_ = self.lstm(out)\n",
    "        out = self.lin_out(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_pred_metrics(y_preds, y, phase: str) -> dict[str, float]:\n",
    "        y_preds = y_preds.view(-1, y_preds.shape[-1])\n",
    "        y = y.view(-1).squeeze()\n",
    "\n",
    "        loss = F.cross_entropy(input=y_preds, target=y, ignore_index=-100)\n",
    "\n",
    "        non_pad_inds = [i for i, cls_i in enumerate(y) if cls_i != -100]\n",
    "\n",
    "        per_cls_recall = tF.recall(\n",
    "            preds=y_preds[non_pad_inds, ...],\n",
    "            target=y[non_pad_inds],\n",
    "            num_classes=4,\n",
    "            average=None,\n",
    "        )\n",
    "\n",
    "        per_cls_precision = tF.precision(\n",
    "            preds=y_preds[non_pad_inds, ...],\n",
    "            target=y[non_pad_inds],\n",
    "            num_classes=4,\n",
    "            average=None,\n",
    "        )\n",
    "\n",
    "        macro_precision = float(per_cls_precision.mean().item())\n",
    "        macro_recall = float(per_cls_recall.mean().item())\n",
    "        macro_f1_score = (\n",
    "            2.0 * macro_precision * macro_recall / (1e-8 + macro_precision + macro_recall)\n",
    "        )\n",
    "\n",
    "        out = {\n",
    "            f\"{(phase + '_') if phase != 'train' else ''}loss\": loss,\n",
    "            **{f\"{phase}_cls_{i}_precision\": float(val) for i, val in enumerate(per_cls_precision)},\n",
    "            **{f\"{phase}_cls_{i}_recall\": float(val) for i, val in enumerate(per_cls_recall)},\n",
    "            f\"{phase}_macro_precision\": macro_precision,\n",
    "            f\"{phase}_macro_recall\": macro_recall,\n",
    "            f\"{phase}_macro_f1_score\": macro_f1_score,\n",
    "        }\n",
    "\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def _agg_stats(step_outputs):\n",
    "        out = {}\n",
    "        agg_items = collections.defaultdict(list)\n",
    "\n",
    "        for items in step_outputs:\n",
    "            for key, val in items.items():\n",
    "                if not isinstance(val, torch.Tensor):\n",
    "                    val = torch.tensor(val)\n",
    "\n",
    "                agg_items[key].append(val)\n",
    "\n",
    "        for key, vals in agg_items.items():\n",
    "            avg_vals = float(torch.stack(vals).mean().item())\n",
    "            out[f\"avg_{key}\"] = avg_vals\n",
    "\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx: int):\n",
    "        X, y = batch\n",
    "        y_preds = self.forward(X)\n",
    "\n",
    "        out = self._compute_pred_metrics(y_preds, y, phase=\"train\")\n",
    "\n",
    "        self.log_dict(\n",
    "            out,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "        return out\n",
    "\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        out = self._agg_stats(training_step_outputs)\n",
    "\n",
    "        self.log_dict(\n",
    "            out,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "    def validation_step(self, batch, batch_idx: int):\n",
    "        X, y = batch\n",
    "        y_preds = self.forward(X)\n",
    "\n",
    "        out = self._compute_pred_metrics(y_preds, y, phase=\"val\")\n",
    "\n",
    "        self.log_dict(\n",
    "            out,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "        return out\n",
    "\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        out = self._agg_stats(validation_step_outputs)\n",
    "\n",
    "        self.log_dict(\n",
    "            out,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "    def test_step(self, batch, batch_idx: int):\n",
    "        X, y = batch\n",
    "        y_preds = self.forward(X)\n",
    "\n",
    "        out = self._compute_pred_metrics(y_preds, y, phase=\"test\")\n",
    "\n",
    "        self.log_dict(\n",
    "            out,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "        return out\n",
    "\n",
    "    def test_epoch_end(self, test_step_outputs):\n",
    "        out = self._agg_stats(test_step_outputs)\n",
    "\n",
    "        self.log_dict(\n",
    "            out,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        df_train = HFDataset(self.hfdf[\"train\"])\n",
    "\n",
    "        train_dataloader = torch.utils.data.DataLoader(\n",
    "            dataset=df_train,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=8,\n",
    "            collate_fn=self.fn_pad_sequences,\n",
    "        )\n",
    "\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        df_eval = HFDataset(self.hfdf[\"eval\"])\n",
    "\n",
    "        eval_dataloader = torch.utils.data.DataLoader(\n",
    "            dataset=df_eval,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=8,\n",
    "            collate_fn=self.fn_pad_sequences,\n",
    "        )\n",
    "\n",
    "        return eval_dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        df_test = HFDataset(self.hfdf[\"test\"])\n",
    "\n",
    "        test_dataloader = torch.utils.data.DataLoader(\n",
    "            dataset=df_test,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=8,\n",
    "            collate_fn=self.fn_pad_sequences,\n",
    "        )\n",
    "\n",
    "        return test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de7693b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T01:23:14.367003Z",
     "start_time": "2022-03-17T23:47:02.394962Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    configs = [\n",
    "        (512, 32),\n",
    "        (256, 48),\n",
    "        (128, 64),\n",
    "    ]\n",
    "\n",
    "    for hidden_size, batch_size in configs:\n",
    "        accumulate_grad_batches = 128 // batch_size\n",
    "\n",
    "        model = LitSegmenterBaseline(\n",
    "            hidden_size=hidden_size,\n",
    "            batch_size=batch_size,\n",
    "            tokenizer_uri=\"../tokenizers/6000_subwords/tokenizer.json\",\n",
    "            dataset_uri=\"../data/df_tokenized_split_0_120000_6000\",\n",
    "        )\n",
    "\n",
    "        trainer = pl.Trainer.from_argparse_args(\n",
    "            args,\n",
    "            overfit_batches=0.001 if DEBUG_RUN else 0.0,\n",
    "            accumulate_grad_batches=accumulate_grad_batches,\n",
    "        )\n",
    "\n",
    "        trainer.fit(model)\n",
    "\n",
    "        if not DEBUG_RUN:\n",
    "            trainer.test()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser = pl.Trainer.add_argparse_args(parser)\n",
    "\n",
    "    args = parser.parse_args(\n",
    "        \"\"\"\n",
    "        --gpu 1\n",
    "        --max_epochs 3\n",
    "        --log_every_n_steps 1000\n",
    "        --precision 32\n",
    "    \"\"\".split()\n",
    "    )\n",
    "\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
