{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3bf320b",
   "metadata": {},
   "source": [
    "## Build segmenter model tokenizer\n",
    "\n",
    "- Tokenizer dictionary is built using a corpus from the Brazilian legal domain;\n",
    "- Dictionary has 6000 tokens (20% the size of Bertimbau's dictionary size);\n",
    "- No tokenization preprocessing (such as text normalization, diacritic removal, case folding) was employed;\n",
    "- BERT post-processing template injection and special tokens ([CLS], [SEP], [UNK] tokens) are maintained, keeping our segmenter model compatible with other popular transformer-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5484dfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T03:19:04.320758Z",
     "start_time": "2022-03-08T03:19:04.317941Z"
    }
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import regex\n",
    "\n",
    "TOKENIZER_OUTPUT_DIR = \"../tokenizers\"\n",
    "\n",
    "pathlib.Path(TOKENIZER_OUTPUT_DIR).mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1894ca34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T03:02:05.663216Z",
     "start_time": "2022-03-08T03:02:02.245737Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    \"neuralmind/bert-base-portuguese-cased\",\n",
    "    local_files_only=True,\n",
    "    cache_dir=\"../cache/tokenizers\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d621e4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T03:02:48.580658Z",
     "start_time": "2022-03-08T03:02:17.501058Z"
    }
   },
   "outputs": [],
   "source": [
    "UPPERCASE_LETTERS = \"ÀÁÂÃÇÉÊẼÓÕÔÜÚÍA-Z\\u0303\\u0300\\u0301\\u0302\\u0303\\u0304\\u0305\\u0340\\u0341\\u0342\\u0343\"\n",
    "\n",
    "\n",
    "RE_JUSTIFICATIVA = regex.compile(\n",
    "    r\"\\s*(?:\" +\n",
    "    r\"J\\s*U\\s*S\\s*T\\s*I\\s*F\\s*I\\s*C\\s*A?\\s*T\\s*I\\s*V\\s*A|\" +\n",
    "    r\"J\\s*u\\s*s\\s*t\\s*i\\s*f\\s*i\\s*c\\s*a\\s*t\\s*i\\s*v\\s*a\\s+(?=[\" + UPPERCASE_LETTERS + r\"])|\" +\n",
    "    r\"J\\s*U\\s*S\\s*T\\s*I\\s*F\\s*I\\s*C\\s*A\\s*[CÇ]\\s*[AÂÃÀÁ]\\s*O|\" +\n",
    "    r\"J\\s*u\\s*s\\s*t\\s*i\\s*f\\s*i\\s*c\\s*a\\s*[cç]\\s*[aãâàá]\\s*o\\s+(?=[\" + UPPERCASE_LETTERS + r\"])\" +\n",
    "    r\")\"\n",
    ")\n",
    "\n",
    "RE_ANEXO = regex.compile(r\"\\s*A\\s*N\\s*E\\s*X\\s*O\")\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"../data/content.txt\",\n",
    "    usecols=[\"imgArquivoTeorPDF\"],\n",
    "    header=0,\n",
    "    index_col=None,\n",
    ").squeeze(\"columns\")\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "good_inds = [i for i, text in enumerate(df) if isinstance(text, str) and 10 <= len(text)]\n",
    "df = df.iloc[good_inds]\n",
    "\n",
    "df = df.map(lambda item: RE_JUSTIFICATIVA.split(item)[0])\n",
    "df = df.map(lambda item: RE_ANEXO.split(item)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faca2864",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T03:18:53.865372Z",
     "start_time": "2022-03-08T03:18:34.377353Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = tokenizer.train_new_from_iterator(df, vocab_size=6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87a82a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T03:19:09.073786Z",
     "start_time": "2022-03-08T03:19:09.063466Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(TOKENIZER_OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
