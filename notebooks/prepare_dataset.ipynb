{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6f59ec98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-17T22:09:18.154192Z",
     "start_time": "2022-02-17T22:09:18.151742Z"
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "import nltk\n",
    "import tokenizers\n",
    "import regex\n",
    "\n",
    "import segmentador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5461fd0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-17T20:46:40.849376Z",
     "start_time": "2022-02-17T20:46:36.718875Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "seg_model = segmentador.Segmenter(local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "d8643074",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T01:15:12.279374Z",
     "start_time": "2022-02-18T01:15:12.266823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marker symbol (valid): ✓\n",
      "Marker symbol (noise): ❌s__ ❌e__\n"
     ]
    }
   ],
   "source": [
    "RE_NOISE_BLOCKS = (\n",
    "    regex.compile(\n",
    "        r\"((PL|PDL|PEC)\\s*n[\\.o\\sº]*[\\d\\s]+/[\\s\\d]+)?+\\s*\"\n",
    "        r\"A\\s*p\\s*r\\s*e\\s*s\\s*e\\s*n\\s*t\\s*a\\s*[cç]\\s*[aã]\\s*o\\s*:\"\n",
    "        r\"(\\s*\\d\\s*){2}/(\\s*\\d\\s*){2}/(\\s*\\d\\s*){6}:(\\s*\\d){2}\",\n",
    "        regex.IGNORECASE | regex.MULTILINE,\n",
    "    ),\n",
    "    regex.compile(r\"([0-9]{9,})\"),\n",
    ")\n",
    "\n",
    "QUOTES = r\"”“\\\"'\"\n",
    "\n",
    "STANDARD_PREFIXES = (\n",
    "    r\"(?<=^|;(?:\\s*e|\\s*ou)?|[\\.:\\?]|\\(\\s*NR\\s*\\)|\" +\n",
    "    f\"[{QUOTES}]|\" + \n",
    "    \"|\".join(reg.pattern for reg in RE_NOISE_BLOCKS) +\n",
    "    \")\"\n",
    ")\n",
    "\n",
    "RE_PRE_BLOCKS = tuple(\n",
    "    regex.compile(f\"{STANDARD_PREFIXES}(?=\\s*{pattern})\", regex.IGNORECASE)\n",
    "    for pattern in [\n",
    "        r\"§\\s*[0-9]+\",\n",
    "        r\"Art(?:igo)?s?\\s*\\.?\\s*(?:[-–0-9A-Z]+|\\.{3}|[uú]nico)\",\n",
    "        r\"(?: [A-Za-z]|[0-9]{1,2})\\s*\\)\",\n",
    "        r\"par[áa]grafo\\s*[úu]nico\",\n",
    "        r\"cap[ií]tulo\",\n",
    "        r\"(?:sub)?se[çc][ãa]o\",\n",
    "        r\"\\(?M{0,3}(?:C[MD]|D?C{0,3})(?:X[CL]|L?X{0,3})(?:I?X|I?V|V?I{1,3})\\s*(?:–|-|\\))\",\n",
    "        r\"\\(?\\s+[0-9]{1,2}[\\sº]*[-–\\)\\.]\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "DEPT_EXTENSION_CORE = (\n",
    "    r\"(?:(?:Sra?|Senhora?)?[\\s\\.]*(?:Deputad[oa]|Dep.)|\" +\n",
    "    r\"(?:Sra?|Senhora?)[\\s\\.]*(?:Deputad[oa]|Dep.)?)\\s*\"\n",
    ")\n",
    "\n",
    "DEPT_EXTENSION_A = (\n",
    "    r\"[^\\(]*?\\(\\s*D[oa]\\s*\" +\n",
    "    DEPT_EXTENSION_CORE +\n",
    "    f\"(?:[^{QUOTES}\\)]+?\\))?\"\n",
    ")\n",
    "\n",
    "DEPT_EXTENSION_B = (\n",
    "    r\".*?D[oa]\\s*\" +\n",
    "    DEPT_EXTENSION_CORE +\n",
    "    f\"(?:[^{QUOTES}]+?(?=[{QUOTES}]))?\"\n",
    ")\n",
    "\n",
    "DEPT_EXTENSION = f\"(?:{DEPT_EXTENSION_A}|{DEPT_EXTENSION_B})\"\n",
    "\n",
    "RE_SPECIAL = (\n",
    "    (regex.compile(\n",
    "        r\"(REQUERIMENTO\\s*DE\\s*INFORMA[cÇ][oÕ]ES.*?(?:DE\\s*[\\.0-9]+|N[\\.\\s]*[oº](?:[^,]*?,\\s*DE\\s*[\\.0-9]+)?)\" +\n",
    "        f\"(?:{DEPT_EXTENSION})?\" +\n",
    "        r\")\\s*\" +\n",
    "        \"(.+?)(?=(Excelent[ií]ssim[oa])?\\s*(?:Senhora?|Sra?)[\\.\\s*]Presidente)\", regex.IGNORECASE),\n",
    "     lambda symb: f\" {symb} \" + r\"\\1\" + f\" {symb} \" + r\"\\2\" + f\" {symb} \"),\n",
    ")\n",
    "\n",
    "RE_PRE_POST_BLOCKS = (\n",
    "    regex.compile(r\"(O\\s+Congresso\\s+Nacional\\s+decreta:)\", regex.IGNORECASE),\n",
    "    regex.compile(r\"(Projeto\\s*de\\s*Lei\" + DEPT_EXTENSION + \")\", regex.IGNORECASE),\n",
    ")\n",
    "\n",
    "MARKER_VALID = \"\\u2713\"\n",
    "MARKER_NOISE_START = \"\\u274Cs__\"\n",
    "MARKER_NOISE_END = \"\\u274Ce__\"\n",
    "SPECIAL_SYMBOLS = {\n",
    "    MARKER_VALID: 1,\n",
    "    MARKER_NOISE_START: 2,\n",
    "    MARKER_NOISE_END: 3,\n",
    "}\n",
    "\n",
    "print(\"Marker symbol (valid):\", MARKER_VALID)\n",
    "print(\"Marker symbol (noise):\", MARKER_NOISE_START, MARKER_NOISE_END)\n",
    "\n",
    "def regex_legal_item_anymatch(text: str) -> str:\n",
    "    for reg in RE_PRE_BLOCKS:\n",
    "        text = reg.sub(f\" {MARKER_VALID} \", text, concurrent=True)\n",
    "    \n",
    "    for reg in RE_NOISE_BLOCKS:\n",
    "        text = reg.sub(f\" {MARKER_NOISE_START} \" + r\"\\1\" + f\" {MARKER_NOISE_END} \", text, concurrent=True)\n",
    "        \n",
    "    for reg in RE_PRE_POST_BLOCKS:\n",
    "        text = reg.sub(f\" {MARKER_VALID} \" + r\"\\1\" + f\" {MARKER_VALID} \", text, concurrent=True)\n",
    "    \n",
    "    for reg, fun in RE_SPECIAL:\n",
    "        text = reg.sub(fun(MARKER_VALID), text, concurrent=True)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "1511c213",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T01:17:38.092350Z",
     "start_time": "2022-02-18T01:17:33.271373Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-998d1132630bd9b4\n",
      "Reusing dataset csv (../cache/datasets/csv/default-998d1132630bd9b4/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb17b4b062e4df9a61733af8875c2da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at ../cache/datasets/csv/default-998d1132630bd9b4/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-e128a25163b7a735.arrow\n",
      "Loading cached processed dataset at ../cache/datasets/csv/default-998d1132630bd9b4/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-7f360c383a5b8cd9.arrow\n",
      "Loading cached processed dataset at ../cache/datasets/csv/default-998d1132630bd9b4/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-4fbd80db7c82a144.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4bd5c7ae094140b9ed02f622b4807e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = datasets.load_dataset(\n",
    "    \"csv\",\n",
    "    data_files=[\"../data/content.txt\"],\n",
    "    header=None,\n",
    "    names=[\"text\"],\n",
    "    cache_dir=\"../cache/datasets\",\n",
    "    nrows=10010,\n",
    ")\n",
    "\n",
    "RE_JUSTIFICATIVA = regex.compile(r\"\\s*JUSTIFICATIVA\")\n",
    "\n",
    "df = df.filter(lambda item: isinstance(item[\"text\"], str) and len(item[\"text\"]) >= 128)\n",
    "df = df.filter(lambda item: \"JUSTIFICATIVA\" in item[\"text\"])\n",
    "df = df.map(lambda item: {\"text\": RE_JUSTIFICATIVA.split(item[\"text\"])[0]})\n",
    "\n",
    "# df = df.filter(lambda item: RE_SPECIAL[0][0].search(seg_model.preprocess_legal_text(item[\"text\"])) is not None)\n",
    "\n",
    "# tokenizers.pre_tokenizers.Sequence([\n",
    "#     tokenizers.pre_tokenizers.Whitespace(),\n",
    "#     tokenizers.pre_tokenizers.Punctuation(),\n",
    "# ])\n",
    "\n",
    "def preprocess_instance(item, ind):\n",
    "    preprocessed_text = seg_model.preprocess_legal_text(item[\"text\"])\n",
    "    preprocessed_text = regex_legal_item_anymatch(preprocessed_text)\n",
    "    tokens = nltk.tokenize.word_tokenize(preprocessed_text, language=\"portuguese\")\n",
    "    \n",
    "    labels = [0] * len(tokens)\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(tokens) - 1:\n",
    "        if tokens[i] in SPECIAL_SYMBOLS:\n",
    "            token = tokens.pop(i)\n",
    "            labels.pop(i)\n",
    "            labels[i] = SPECIAL_SYMBOLS[token]\n",
    "            continue\n",
    "            \n",
    "        i += 1\n",
    "\n",
    "    ret = {\n",
    "        \"id\": str(ind),\n",
    "        \"labels\": labels,\n",
    "        \"tokens\": tokens,\n",
    "    }\n",
    "    \n",
    "    return ret\n",
    "\n",
    "df = df.map(preprocess_instance, with_indices=True, num_proc=1, remove_columns=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "a80d2507",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T01:18:10.527226Z",
     "start_time": "2022-02-18T01:18:10.519907Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJETO DE LEI COMPLEMENTAR Nº , DE 2002 ( Da Sra. Tânia Soares ) Altera a Lei Complementar nº 87 , de 13 de setembro de 1996 , que dispõe sobre o imposto dos Estados e do Distrito Federal sobre operações relativas à circulação de mercadorias e sobre prestações de serviços de transporte interestadual e intermunicipal e de comunicação . O Congresso Nacional decreta : Art . 1º O parágrafo 5º do artigo 8º da Lei Complementar nº 87 , de 13 de setembro de 1996 passa a vigorar com a seguinte redação , incluindo nele , ainda , um novo parágrafo 6º : “ § 5º O imposto a ser pago por substituição tributária , na hipótese do inciso II do caput , corresponderá à diferença entre o valor resultante da aplicação da alíquota prevista para as operações ou prestações internas do Estado de destino sobre a respectiva base de cálculo e o valor do imposto devido pela operação ou prestação própria do substituto , deduzindo-se dessa diferença um desconto igual à taxa de encargos de atualização aplicável ao pagamento do tributo , proporcional à duração estimada do giro de estoque segundo a cada tipo de estabelecimento. ” ( NR ) “ § 6º A duração do giro do estoque previsto no parágrafo anterior será calculado , para cada tipo de estabelecimento , levando-se em consideração o prazo médio de permanência dos bens tributados em estoque , obtidos por levantamento no mercado , ainda que por amostragem , ou através de informações e outros elementos fornecidos por entidades representativas dos respectivos setores. ” ( AC ) Art . 2º O artigo 10 e o seu parágrafo 1º da Lei Complementar nº 87 , de 13 de setembro de 1996 , passam a vigorar com a seguinte redação : “ Art . 10 . É assegurado ao contribuinte substituído o direito à restituição do valor do imposto pago por força da substituição tributária , correspondente ao fato gerador presumido que não se realizar ou que venha a se realizar por valor inferior ao estipulado nos termos do inciso II do art . 8º . § 1º Formulado o pedido de restituição e não havendo deliberação no prazo de sessenta dias , o contribuinte substituído poderá se creditar , em sua escrita fiscal , do valor objeto do pedido , devidamente atualizado segundo os mesmos critérios aplicáveis ao tributo . Art . 3º Esta Lei Complementar entra em vigor na data de sua publicação .\n",
      "\n",
      "\n",
      "\n",
      "PROJETO DE LEI COMPLEMENTAR Nº , DE 2002 ( Da Sra. Tânia Soares )\n",
      "\n",
      "Altera a Lei Complementar nº 87 , de 13 de setembro de 1996 , que dispõe sobre o imposto dos Estados e do Distrito Federal sobre operações relativas à circulação de mercadorias e sobre prestações de serviços de transporte interestadual e intermunicipal e de comunicação .\n",
      "\n",
      "O Congresso Nacional decreta :\n",
      "\n",
      "Art . 1º O parágrafo 5º do artigo 8º da Lei Complementar nº 87 , de 13 de setembro de 1996 passa a vigorar com a seguinte redação , incluindo nele , ainda , um novo parágrafo 6º : “\n",
      "\n",
      "§ 5º O imposto a ser pago por substituição tributária , na hipótese do inciso II do caput , corresponderá à diferença entre o valor resultante da aplicação da alíquota prevista para as operações ou prestações internas do Estado de destino sobre a respectiva base de cálculo e o valor do imposto devido pela operação ou prestação própria do substituto , deduzindo-se dessa diferença um desconto igual à taxa de encargos de atualização aplicável ao pagamento do tributo , proporcional à duração estimada do giro de estoque segundo a cada tipo de estabelecimento. ” ( NR ) “\n",
      "\n",
      "§ 6º A duração do giro do estoque previsto no parágrafo anterior será calculado , para cada tipo de estabelecimento , levando-se em consideração o prazo médio de permanência dos bens tributados em estoque , obtidos por levantamento no mercado , ainda que por amostragem , ou através de informações e outros elementos fornecidos por entidades representativas dos respectivos setores. ” ( AC ) Art . 2º O artigo 10 e o seu parágrafo 1º da Lei Complementar nº 87 , de 13 de setembro de 1996 , passam a vigorar com a seguinte redação : “\n",
      "\n",
      "Art .\n",
      "\n",
      "10 . É assegurado ao contribuinte substituído o direito à restituição do valor do imposto pago por força da substituição tributária , correspondente ao fato gerador presumido que não se realizar ou que venha a se realizar por valor inferior ao estipulado nos termos do inciso II do art .\n",
      "\n",
      "8º .\n",
      "\n",
      "§ 1º Formulado o pedido de restituição e não havendo deliberação no prazo de sessenta dias , o contribuinte substituído poderá se creditar , em sua escrita fiscal , do valor objeto do pedido , devidamente atualizado segundo os mesmos critérios aplicáveis ao tributo .\n",
      "\n",
      "Art . 3º Esta Lei Complementar entra em vigor na data de sua publicação .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_results(df, id_):\n",
    "    tokens = df[\"train\"][id_][\"tokens\"]\n",
    "    labels = df[\"train\"][id_][\"labels\"]\n",
    "    \n",
    "    print(\" \".join(df[\"train\"][id_][\"tokens\"]))\n",
    "    print()\n",
    "    \n",
    "    sentence = []\n",
    "    \n",
    "    for tok, lab in zip(tokens, labels):\n",
    "        if lab == SPECIAL_SYMBOLS[MARKER_VALID]:\n",
    "            print(\" \".join(sentence), end=\"\\n\\n\")\n",
    "            sentence = []\n",
    "\n",
    "        sentence.append(\n",
    "            f\"@@{tok}@@\"\n",
    "            if (lab == SPECIAL_SYMBOLS[MARKER_NOISE_START] or lab == SPECIAL_SYMBOLS[MARKER_NOISE_END])\n",
    "            else tok\n",
    "        )\n",
    "    \n",
    "    print(\" \".join(sentence), end=\"\\n\\n\")\n",
    "            \n",
    "print_results(df, 49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f6f7fd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T15:08:20.098638Z",
     "start_time": "2022-02-15T15:07:49.613833Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    # source: https://huggingface.co/docs/transformers/custom_datasets#preprocess\n",
    "    tokenized_inputs = seg_model.tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    \n",
    "    for i, label in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    \n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "df_tokenized = df.map(tokenize_and_align_labels, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f2eca53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T15:08:20.163644Z",
     "start_time": "2022-02-15T15:08:20.100565Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'labels', 'tokens', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 15993\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['id', 'labels', 'tokens', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1999\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'labels', 'tokens', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokenized_train_eval_test = df_tokenized[\"train\"].train_test_split(test_size=0.2, shuffle=True, seed=16)\n",
    "df_tokenized_test_eval = df_tokenized_train_eval_test[\"test\"].train_test_split(test_size=0.5, shuffle=False)\n",
    "df_tokenized_split = datasets.DatasetDict({\n",
    "    \"train\": df_tokenized_train_eval_test[\"train\"],\n",
    "    \"eval\": df_tokenized_test_eval[\"train\"],\n",
    "    \"test\": df_tokenized_test_eval[\"test\"],\n",
    "})\n",
    "# df_tokenized_split.save_to_disk(\"../data/df_tokenized_split\")\n",
    "df_tokenized_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eec224ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T15:08:35.091656Z",
     "start_time": "2022-02-15T15:08:35.086936Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='string', id=None),\n",
       " 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokenized_split[\"train\"].features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
