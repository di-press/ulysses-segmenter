{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f59ec98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T00:32:56.357949Z",
     "start_time": "2022-02-21T00:32:56.355218Z"
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "import nltk\n",
    "import tokenizers\n",
    "import regex\n",
    "import colorama\n",
    "import random\n",
    "\n",
    "random.seed(16)\n",
    "\n",
    "import segmentador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5461fd0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-20T18:32:57.356148Z",
     "start_time": "2022-02-20T18:32:53.343032Z"
    }
   },
   "outputs": [],
   "source": [
    "seg_model = segmentador.Segmenter(local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d9fdeb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T00:33:39.825740Z",
     "start_time": "2022-02-21T00:33:39.820452Z"
    }
   },
   "outputs": [],
   "source": [
    "test_cases = {}\n",
    "\n",
    "\n",
    "def test_regex(segments):\n",
    "    correct = 0\n",
    "    skipped = 0\n",
    "    incorrect = []\n",
    "    \n",
    "    if not test_cases:\n",
    "        return\n",
    "    \n",
    "    for idx, expected in test_cases.items():\n",
    "        if idx >= len(segments):\n",
    "            skipped += 1\n",
    "            continue\n",
    "            \n",
    "        total_segments = int(len(segments[idx]) > 0)\n",
    "        total_segments += sum([lab == SPECIAL_SYMBOLS[MARKER_VALID] for lab in segments[idx]])\n",
    "        if total_segments == expected:\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect.append(str(idx))\n",
    "            \n",
    "    correct_prop = correct / len(test_cases)\n",
    "    \n",
    "    print(\n",
    "        f\"Correct proportion: {100. * correct_prop:.2f}% ({correct} of {len(test_cases)})\" +\n",
    "        (f\", {skipped} tests skipped.\" if skipped > 0 else \"\")\n",
    "    )\n",
    "    assert correct == len(test_cases) - skipped, f\"Incorrect: {', '.join(incorrect)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8643074",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T01:52:39.843241Z",
     "start_time": "2022-02-21T01:52:39.810676Z"
    }
   },
   "outputs": [],
   "source": [
    "MARKER_VALID = \"\\u2713\"\n",
    "MARKER_NOISE_START = \"\\u274Cs__\"\n",
    "MARKER_NOISE_END = \"\\u274Ce__\"\n",
    "\n",
    "SPECIAL_SYMBOLS = {\n",
    "    MARKER_VALID: 1,\n",
    "    MARKER_NOISE_START: 2,\n",
    "    MARKER_NOISE_END: 3,\n",
    "}\n",
    "\n",
    "print(\"Marker symbol (valid):\", MARKER_VALID)\n",
    "print(\"Marker symbol (noise):\", MARKER_NOISE_START, MARKER_NOISE_END)\n",
    "\n",
    "class DetectRecurrentNoise:\n",
    "    # PL – Rótulos e embalagens – percentuais impostos 1 CÂMARA DOS DEPUTADOS\n",
    "    \n",
    "    RE_BARCODE = regex.compile(r\"\\*([\\sA-Z0-9]+)\\*\")\n",
    "    RE_PREAMBLE = regex.compile(\n",
    "        r\"^\\s*(.{,60}?)[\\s0-9]*\" +\n",
    "        \"(?=C[aâ]mara\\s*dos\\s*deputados\\s*(Proj|Req))\",\n",
    "        regex.IGNORECASE,\n",
    "    )\n",
    "    RE_BLANK_SPACES = regex.compile(r\"\\s+\")\n",
    "    \n",
    "    @classmethod\n",
    "    def _detect_barcode(cls, subpattern, text):\n",
    "        pseudo_patterns = cls.RE_BARCODE.findall(text)\n",
    "        \n",
    "        if not pseudo_patterns:\n",
    "            return text\n",
    "        \n",
    "        pseudo_patterns = sorted(set(pseudo_patterns))\n",
    "        \n",
    "        for pseudo_pattern in pseudo_patterns:\n",
    "            pattern = list(cls.RE_BLANK_SPACES.sub(\"\", pseudo_pattern))\n",
    "            pattern.append(\"\")\n",
    "            pattern.insert(0, \"\")\n",
    "            pattern = r\"\\s*\".join(pattern)\n",
    "            \n",
    "            text = regex.sub(r\"(\\*\" + pattern + r\"\\*\" + pattern + \")\", subpattern, text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    @classmethod\n",
    "    def _detect_preamble_noise(cls, subpattern, text):\n",
    "        preamble = cls.RE_PREAMBLE.match(text)\n",
    "    \n",
    "        if not preamble or not preamble.group(1).strip():\n",
    "            return text\n",
    "        \n",
    "        preamble_content = r\"\\s*\".join(preamble.group(1).split(\" \"))\n",
    "        preamble_content = preamble_content.replace(\")\", r\"\\)\")\n",
    "        preamble_content = preamble_content.replace(\"(\", r\"\\(\")\n",
    "        text = regex.sub(r\"(\\s*\" + preamble_content + r\"[\\s\\d]*)\", subpattern, text)\n",
    "        return text\n",
    "    \n",
    "    @classmethod\n",
    "    def sub(cls, subpattern: str, text: str, *args, **kwargs):\n",
    "        text = cls._detect_barcode(subpattern, text)\n",
    "        text = cls._detect_preamble_noise(subpattern, text)\n",
    "        return text\n",
    "\n",
    "\n",
    "RE_NOISE_BLOCKS = (\n",
    "    regex.compile(\n",
    "        r\"((PL|PDL|PEC)\\s*n[\\.o\\sº]*[\\d\\s]+/[\\s\\d]+)?+\\s*\"\n",
    "        r\"A\\s*p\\s*r\\s*e\\s*s\\s*e\\s*n\\s*t\\s*a\\s*[cç]\\s*[aã]\\s*o\\s*:\"\n",
    "        r\"(\\s*\\d\\s*){2}/(\\s*\\d\\s*){2}/(\\s*\\d\\s*){6}:(\\s*\\d){2}\",\n",
    "        regex.IGNORECASE | regex.MULTILINE,\n",
    "    ),\n",
    "    regex.compile(r\"([0-9]{9,})\"),\n",
    "    regex.compile(r\"(_{9,})\"),\n",
    "    regex.compile(r\"(^[\\s0-9]+|[\\s0-9]+$)\"),\n",
    ")\n",
    "\n",
    "QUOTES = r\"”“\\\"'\"\n",
    "\n",
    "STANDARD_PREFIXES = (\n",
    "    r\"(?<=^|;(?:\\s*e|\\s*ou)?|[\\.:\\?]|\\(\\s*(?:NR|AC)\\s*\\)|\" +\n",
    "    f\"[{QUOTES}]|\" + \n",
    "    \"|\".join(reg.pattern for reg in RE_NOISE_BLOCKS) +\n",
    "    f\"|{MARKER_NOISE_START}.*?{MARKER_NOISE_END}\\s*\" +\n",
    "    \")\"\n",
    ")\n",
    "OPTIONAL_PREFIX_EXTENSIONS = (\n",
    "    r\"(?<=\\s*(?:\" +\n",
    "    r\"[0-9]{,2}\" +\n",
    "    r\")\\s*)\"\n",
    ")\n",
    "\n",
    "VALID_ROMAN_NUM = r\"M{0,3}(?:C[MD]|D?C{0,3})(?:X[CL]|L?X{0,3})(?:I?X|I?V|V?I{1,3})\"\n",
    "\n",
    "RE_PRE_BLOCKS = tuple(\n",
    "    regex.compile(f\"{STANDARD_PREFIXES}{OPTIONAL_PREFIX_EXTENSIONS}(?=\\s*{pattern})\", regex.IGNORECASE)\n",
    "    for pattern in [\n",
    "        r\"§\\s*[0-9]+\",\n",
    "        r\"Art(?:igo)?s?\\s*\\.?\\s*(?:[-–º0-9A-Z]+|\\.{3}|[uú]nico)\",\n",
    "        r\"(?:\\s[A-Za-z]|[0-9]{1,2})\\s*\\)\",\n",
    "        r\"(?:par[áa]grafo|§)\\s*[úu]nico\",\n",
    "        r\"(?:par[áa]grafo|§)\\s*[0-9]{1,2}[\\so0º]*:\",\n",
    "        r\"(?:sub)?se[çc][ãa]o\",\n",
    "        r\"\\(?\" + f\"{VALID_ROMAN_NUM}\" + r\"\\s*(?:[-–\\)\\.])\",\n",
    "        r\"(?<!Art(?:igo)?\\s?\\.?\\s?)\\(?\\s+[0-9]{1,2}[\\s0oº]*(?:[-–\\)]|\\.(?![0-9]))\",\n",
    "        r\"Sala\\s*das?\\s*(Sessões|comiss[aã]o|Reuni[oõ]es).{,200}$\",\n",
    "        r\"Senado\\s*Federal\\s*,.{,200}$\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "DEPT_EXTENSION_CORE = (\n",
    "    r\"(?:(?:Sra?|Senhora?)?[\\s\\.]*(?:Deputad[oa]|Dep\\.)|\" +\n",
    "    r\"(?:Sra?|Senhora?)[\\s\\.]*(?:Deputad[oa]|Dep\\.)?|mesa\\s*diretora)\" +\n",
    "    r\"\\s*\"\n",
    ")\n",
    "\n",
    "DEPT_EXTENSION_A = (\n",
    "    r\"[^\\(]{,100}\\(\\s*(?:D[oa])?\\s*\" +\n",
    "    DEPT_EXTENSION_CORE +\n",
    "    f\"(?:[^{QUOTES}\\)]\" + r\"{1,100})?\\)\"\n",
    ")\n",
    "\n",
    "DEPT_EXTENSION_B = (\n",
    "    r\".{,100}?D[oa]\\s*\" +\n",
    "    DEPT_EXTENSION_CORE +\n",
    "    f\"(?:[^{QUOTES}]\" + r\"{1,100}\" + f\"?(?=[{QUOTES}]))?\"\n",
    ")\n",
    "\n",
    "DEPT_EXTENSION = f\"(?:{DEPT_EXTENSION_A}|{DEPT_EXTENSION_B})\"\n",
    "DATE_AND_ID = r\"(?:(?:DE\\s*)+?[\\._0-9]+|N[\\.\\s]*[o0º](?:[^,]*?[,\\.]+\\s*(?:DE\\s*)+?[\\._0-9]+)?)\"\n",
    "\n",
    "\n",
    "RE_SPECIAL = (\n",
    "    (regex.compile(\n",
    "        r\"(?<=^.{,250}?)(REQUERIMENTO\\s*DE\\s*INFORMA[cÇ](?:[oÕ]ES|[AÃ]O).{,50}?\" +\n",
    "        DATE_AND_ID +\n",
    "        f\"(?:{DEPT_EXTENSION})?\" +\n",
    "        r\")\\s*\" +\n",
    "        \"(.{,600}?)(?=(?:Excelent[ií]ssim[oa])?\\s*(?:Senhora?|Sra?)[\\.\\s*]Presidente)\", regex.IGNORECASE),\n",
    "     lambda symb, deb: f\" {symb} {deb} \" + r\"\\1\" + f\" {symb} {deb} \" + r\"\\2\" + f\" {symb} {deb} \"),\n",
    "    (regex.compile(\n",
    "        r\"(?<=^.{,250}?)(REQUERIMENTO.{,25}?\" +\n",
    "        DATE_AND_ID +\n",
    "        f\"(?:{DEPT_EXTENSION})\" +\n",
    "        r\")\", regex.IGNORECASE),\n",
    "     lambda symb, deb: f\" {symb} {deb} \" + r\"\\1\" + f\" {symb} {deb} \"),\n",
    "    (regex.compile(\n",
    "        r\"(INDICA[CÇ][AÃ]O.{,50}?\" +\n",
    "        DATE_AND_ID +\n",
    "        f\"(?:{DEPT_EXTENSION})\" +\n",
    "        r\")\\s*\" +\n",
    "        \"(.{,600}?)(?=(?:Excelent[ií]ssim[oa])?\\s*(?:Senhora?|Sra?)[\\.\\s*](?:Presidente|Ministr[oa]))\", regex.IGNORECASE),\n",
    "     lambda symb, deb: f\" {symb} {deb} \" + r\"\\1\" + f\" {symb} {deb} \" + r\"\\2\" + f\" {symb} {deb} \"),\n",
    "    (regex.compile(\n",
    "        r\"(?<=^.{,250}?)((?:PROJETO\\s*DE\\s*)?RESOLU[CÇ][AÃ]O.{,50}?\" + \n",
    "        DATE_AND_ID +\n",
    "        f\"(?:{DEPT_EXTENSION})?\" +\n",
    "        r\")\\s*\" +\n",
    "        r\"(.{,600}?)((?:A\\s*mesa\\s*d)?A\\s*C[âa]mara\\s*dos\\s*deputados[^\\.]*?resolve\\s*:)\", regex.IGNORECASE),\n",
    "     lambda symb, deb: f\" {symb} {deb} \" + r\"\\1\" + f\" {symb} {deb} \" + r\"\\2\" + f\" {symb} {deb} \" + r\"\\3\" + f\" {symb} {deb} \"),\n",
    "    (DetectRecurrentNoise, lambda symb_start, symb_end, deb: f\" {symb_start} {deb} \" + r\"\\1\" + f\" {symb_end} {deb} \"),\n",
    ")\n",
    "\n",
    "RE_PRE_POST_BLOCKS = (\n",
    "    regex.compile(r\"(?<=^.{,250}?)(COMISS[AÃ]O\\s*DE\\s*CI[EÊ]NCIA[\\sE]*TECNOLOGIA[\\s,]*COMUNICA[CÇ][AÃ]O[\\sE]*INFORM[AÁ]TICA)\", regex.IGNORECASE),\n",
    "    regex.compile(r\"(O\\s*Congresso\\s*Nacional\\s*decreta:)\", regex.IGNORECASE),\n",
    "    regex.compile(r\"(A\\s*C[aâ]mara\\s*dos\\s+deputados\\s*decreta:)\", regex.IGNORECASE),\n",
    "    regex.compile(r\"(?<=^.{,250}?)(Projeto\\s*de\\s*Lei\\s*\" + DEPT_EXTENSION + \")\", regex.IGNORECASE),\n",
    "    regex.compile(\n",
    "        r\"(?<=^.{,250}?)(Projeto\\s*de\\s*Decreto\\s*Legislativo\\s*\" +\n",
    "        f\"(?:{DEPT_EXTENSION}|{DATE_AND_ID})\" +\n",
    "        \")\",\n",
    "        regex.IGNORECASE,\n",
    "    ),\n",
    "    regex.compile(\n",
    "        r\"(?<=^.{,250}?)(Proposta\\s*de\\s*emenda\\s*(?:cons?titucional|[aàá]\\s*constitui[çc][ãa]o).*?\" +\n",
    "        f\"(?:{DEPT_EXTENSION})\" +\n",
    "        r\")\",\n",
    "        regex.IGNORECASE,\n",
    "    ),\n",
    "    regex.compile(\n",
    "        r\"(cap[ií]tulo\\s*\" + f\"{VALID_ROMAN_NUM}\" +\n",
    "        r\"(?:[-–\\sA-Za-zçàüáéíóúãõẽôâê0-9]|\" + \n",
    "        f\"{MARKER_NOISE_END}|{MARKER_NOISE_START}\" +\n",
    "        r\")+?\" +\n",
    "        f\"(?=(?:{MARKER_VALID}|Art)))\",\n",
    "        regex.IGNORECASE,\n",
    "    ),\n",
    "    regex.compile(\n",
    "        r\"(t[ií]tulo\\s*\" + f\"{VALID_ROMAN_NUM}\" +\n",
    "        r\"(?:[-–\\sA-Za-zçàüáéíóúãõẽôâê0-9]|\" + \n",
    "        f\"{MARKER_NOISE_END}|{MARKER_NOISE_START}\" +\n",
    "        r\")+?\" +\n",
    "        f\"(?=(?:{MARKER_VALID}|cap[íi]tulo)))\",\n",
    "        regex.IGNORECASE,\n",
    "    ),\n",
    ")\n",
    "\n",
    "def regex_legal_item_anymatch(text: str, debug: bool = False) -> str:\n",
    "    aid = 0\n",
    "    \n",
    "    for i, reg in enumerate(RE_NOISE_BLOCKS, aid):\n",
    "        debug_text = f\"{i}_NOISE\" if debug else \"\"\n",
    "        text = reg.sub(f\" {MARKER_NOISE_START} {debug_text} \" + r\"\\1\" + f\" {MARKER_NOISE_END} {debug_text} \", text, concurrent=True)\n",
    "    \n",
    "    for i, (reg, fun) in enumerate(RE_SPECIAL, aid):\n",
    "        debug_text = f\"{i}_SPECIAL\" if debug else \"\"\n",
    "        try:\n",
    "            pat = fun(MARKER_VALID, debug_text)\n",
    "            \n",
    "        except TypeError:\n",
    "            pat = fun(MARKER_NOISE_START, MARKER_NOISE_END, debug_text)\n",
    "            \n",
    "        text = reg.sub(pat, text, concurrent=True)\n",
    "        \n",
    "    for i, reg in enumerate(RE_PRE_BLOCKS, aid):\n",
    "        debug_text = f\"{i}_PRE\" if debug else \"\"\n",
    "        text = reg.sub(f\" {MARKER_VALID} {debug_text} \", text, concurrent=True)\n",
    "        \n",
    "    for i, reg in enumerate(RE_PRE_POST_BLOCKS, aid):\n",
    "        debug_text = f\"{i}_PRE_POS\" if debug else \"\"\n",
    "        text = reg.sub(f\" {MARKER_VALID} {debug_text} \" + r\"\\1\" + f\" {MARKER_VALID} {debug_text} \", text, concurrent=True)\n",
    "        \n",
    "    return text\n",
    "\n",
    "\n",
    "auxaux=\"b ) todas as medicações prescritas com as dosagens utilizadas ; e c ) registro da quantidade de sangue recebida e dados que permitam identificar sua origem , sorologias efetuadas e prazo de validade ; XVI - ter assegurado , durante as consultas , internações , procedimentos diagnósticos , preventivos , cirúrgicos e terapêuticos e na satisfação de suas necessidades fisiológicas : 4 a ) a integridade física ; b ) a privacidade física ; c ) a individualidade ; d ) o respeito aos seus valores éticos e culturais ; e ) a confidencialidade de toda e qualquer informação pessoal ; e f ) a segurança do procedimento ; XVII - ser acompanhado , sempre que assim o desejar , nas consultas , exames e internações , por pessoa de sua livre escolha ; XVIII – ser acompanhada , se assim o desejar , por pessoa de sua livre escolha , no momento do parto e no pós-parto ; XIX – ter garantida a acessibilidade aos serviços , com o fim das barreiras arquitetônicas e de comunicabilidade , oferecendo condições de atendimento adequadas aos portadores de deficiências ou necessidades especiais ; XX - receber do profissional adequado , auxílio imediato e oportuno para a melhoria do conforto e bem-estar ; XXI - ter um local digno , respeitoso e adequado para o atendimento ; XXII - receber ou recusar assistência moral , psicológica , social ou religiosa ; XXIII - ser prévia e expressamente informado quando o tratamento proposto for experimental ou fizer parte de pesquisa , consentindo a participar de forma livre e esclarecida ; XXIV – ter informações relativas às ações de vigilância sanitária e epidemiológica , bem como sobre fatores de risco que afetem a saúde nos locais de moradia e trabalho ; XXV – ter acesso a anestesia em todas as situações em que esta for indicada , bem como a medicações e procedimentos que possam aliviar a dor e o sofrimento ; XXVI - recusar tratamentos dolorosos ou extraordinários para tentar prolongar a vida ; e 5 XXVII - optar pelo local de morte . § 1º - A criança , ao ser internada , terá em seu prontuário a relação das pessoas que poderão acompanhá-la integralmente durante o período de internação . § 2º - A atenção aos problemas de saúde mental realizar-se-á basicamente no âmbito comunitário , mediante práticas intersetoriais , assistência domiciliar e ambulatorial , sendo a internação utilizada como último recurso terapêutico , em ambiente o menos restritivo possível , objetivando a mais breve recuperação do paciente . Artigo 3º - É vedado aos serviços públicos de saúde e às entidades públicas e privadas conveniadas ou contratadas pelo Poder Público : I - realizar , proceder ou permitir qualquer forma de discriminação entre os usuários dos serviços de saúde ; II – prestar serviços ou ações de saúde discriminatórios , em termos de acesso ou qualidade dos procedimentos , entre usuários do SUS e beneficiários de planos , seguros , contratos ou convênios privados de saúde , próprios ou por eles intermediados ; e III - manter acessos diferenciados para os usuários do Sistema Único de Saúde e quaisquer outros usuários , em face de necessidades de atenção semelhantes . § 1º - O disposto no inciso III deste artigo compreende também as portas de entrada e saída , salas de estar , guichês , locais de agendamento , retirada de exames e locais de espera . Artigo 4º - Os serviços públicos de saúde e as entidades privadas , conveniadas ou contratadas pelo Poder Público , têm que garantir a todos os pacientes e usuários : I - a igualdade de acesso , em idênticas condições , a todo e qualquer procedimento para a assistência à saúde , médico ou não , inclusive administrativo , que se faça necessário e seja oferecido pela instituição ; e II - o atendimento equânime em relação à qualidade dos procedimentos referidos no inciso anterior . 6 Parágrafo único - O direito à igualdade de acesso a todos os serviços , exames , procedimentos e à sua qualidade , nos termos desta lei , é extensivo a autarquias , institutos , fundações , hospitais universitários e demais entidades públicas ou privadas , que recebam , a qualquer título , recursos do Sistema Único de Saúde . Artigo 5o . – As pessoas jurídicas de direito público e as de direito privado participantes ou não do SUS são responsáveis , objetivamente , pelos danos que seus agentes , nessa qualidade , causarem ao indivíduo ou à coletividade \"\n",
    "_=preprocess_instance({\"text\": auxaux}, -1, True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1511c213",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T01:44:57.896819Z",
     "start_time": "2022-02-21T01:44:43.136383Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = datasets.load_dataset(\n",
    "    \"csv\",\n",
    "    data_files=[\"../data/content.txt\"],\n",
    "    header=None,\n",
    "    names=[\"text\"],\n",
    "    cache_dir=\"../cache/datasets\",\n",
    "    nrows=6000,\n",
    ")\n",
    "\n",
    "RE_JUSTIFICATIVA = regex.compile(r\"\\s*(?:JUSTIFICATIVA|JUSTIFICA[CÇ][AÃ]O)\")\n",
    "RE_ANEXO = regex.compile(r\"\\s*ANEXO\")\n",
    "\n",
    "df = df.map(lambda item: {\"text\": RE_JUSTIFICATIVA.split(item[\"text\"])[0]})\n",
    "df = df.map(lambda item: {\"text\": RE_ANEXO.split(item[\"text\"])[0]})\n",
    "df = df.filter(lambda item: isinstance(item[\"text\"], str) and 128 <= len(item[\"text\"]) <= 600000)\n",
    "\n",
    "def preprocess_instance(item, ind, print_preprocessed: bool = False, debug: bool = False):\n",
    "    preprocessed_text = seg_model.preprocess_legal_text(item[\"text\"])\n",
    "    preprocessed_text = regex_legal_item_anymatch(preprocessed_text, debug=debug)\n",
    "    tokens = nltk.tokenize.word_tokenize(preprocessed_text, language=\"portuguese\")\n",
    "    \n",
    "    if print_preprocessed:\n",
    "        print(preprocessed_text)\n",
    "    \n",
    "    labels = [0] * len(tokens)\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(tokens) - 1:\n",
    "        if tokens[i] in SPECIAL_SYMBOLS:\n",
    "            token = tokens.pop(i)\n",
    "            labels.pop(i)\n",
    "            labels[i] = SPECIAL_SYMBOLS[token]\n",
    "            continue\n",
    "            \n",
    "        i += 1\n",
    "    \n",
    "    if labels[0] == SPECIAL_SYMBOLS[MARKER_VALID]:\n",
    "        labels[0] = 0\n",
    "    \n",
    "    if tokens[-1] in SPECIAL_SYMBOLS:\n",
    "        labels.pop()\n",
    "        tokens.pop()\n",
    "\n",
    "    ret = {\n",
    "        \"id\": str(ind),\n",
    "        \"labels\": labels,\n",
    "        \"tokens\": tokens,\n",
    "    }\n",
    "    \n",
    "    return ret\n",
    "\n",
    "df = df.map(preprocess_instance, with_indices=True, num_proc=8, remove_columns=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e608993c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T00:48:14.712036Z",
     "start_time": "2022-02-21T00:48:14.705798Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def print_results(df, id_, filters=None, print_full_text: bool = True):\n",
    "    tokens = df[\"train\"][id_][\"tokens\"]\n",
    "    labels = df[\"train\"][id_][\"labels\"]\n",
    "    \n",
    "    if print_full_text:\n",
    "        print(\" \".join(df[\"train\"][id_][\"tokens\"]))\n",
    "        print(64 * \"_\", end=\"\\n\\n\")\n",
    "    \n",
    "    sentence = []\n",
    "    segment_count = int(len(tokens) > 0)\n",
    "    \n",
    "    c_color = colorama.Fore.LIGHTWHITE_EX\n",
    "    c_end = colorama.Fore.RESET\n",
    "    \n",
    "    if isinstance(filters, str):\n",
    "        filters = {filters}\n",
    "    \n",
    "    if filters is not None and not isinstance(filters, set):\n",
    "        filters = set(filters)\n",
    "    \n",
    "    for tok, lab in zip(tokens, labels):\n",
    "        if lab == SPECIAL_SYMBOLS[MARKER_VALID]:\n",
    "            if (not filters or not filters.isdisjoint(map(str.lower, sentence))):\n",
    "                print(c_color, segment_count, c_end, \" \".join(sentence), end=\"\\n\\n\")\n",
    "                \n",
    "            sentence = []\n",
    "            segment_count += 1\n",
    "        \n",
    "        if lab == SPECIAL_SYMBOLS[MARKER_NOISE_START]:\n",
    "            tok = colorama.Fore.RED + tok\n",
    "            \n",
    "        if lab == SPECIAL_SYMBOLS[MARKER_NOISE_END]:\n",
    "            tok = colorama.Fore.RESET + tok\n",
    "        \n",
    "        sentence.append(tok)\n",
    "       \n",
    "    if sentence and (not filters or not filters.isdisjoint(map(str.lower, sentence))):\n",
    "        print(c_color, segment_count, c_end, \" \".join(sentence), end=\"\\n\\n\")\n",
    "    \n",
    "    print(colorama.Fore.RESET)\n",
    "    print(f\"Idx/Segment count:   {id_}: {segment_count}\")\n",
    "    \n",
    "    return segment_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df3ef9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T01:45:20.742408Z",
     "start_time": "2022-02-21T01:44:57.898433Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(test_cases[3936])\n",
    "print_results(df, 3936, print_full_text=True)\n",
    "test_regex(df[\"train\"][\"labels\"])\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(df.num_rows)\n",
    "\n",
    "randint = 166\n",
    "while randint in test_cases:\n",
    "    randint = random.randint(0, 1 + df[\"train\"].num_rows)\n",
    "\n",
    "print(\"Chosen id:\", randint)\n",
    "segment_count = print_results(df, randint, print_full_text=True)\n",
    "print(\"Is it correct? [y/N]:\", end=\" \")\n",
    "inp = input()\n",
    "if inp == \"y\":\n",
    "    test_cases[randint] = segment_count\n",
    "    print(\"Added to test cases.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85081017",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T17:58:27.781802Z",
     "start_time": "2022-02-18T17:58:25.830858Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"train\"][\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6f7fd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T21:04:02.703423Z",
     "start_time": "2022-02-18T21:03:51.852099Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    # source: https://huggingface.co/docs/transformers/custom_datasets#preprocess\n",
    "    tokenized_inputs = seg_model.tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    \n",
    "    for i, label in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    \n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "df_tokenized = df.map(tokenize_and_align_labels, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2eca53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T15:08:20.163644Z",
     "start_time": "2022-02-15T15:08:20.100565Z"
    }
   },
   "outputs": [],
   "source": [
    "df_tokenized_train_eval_test = df_tokenized[\"train\"].train_test_split(test_size=0.2, shuffle=True, seed=16)\n",
    "df_tokenized_test_eval = df_tokenized_train_eval_test[\"test\"].train_test_split(test_size=0.5, shuffle=False)\n",
    "df_tokenized_split = datasets.DatasetDict({\n",
    "    \"train\": df_tokenized_train_eval_test[\"train\"],\n",
    "    \"eval\": df_tokenized_test_eval[\"train\"],\n",
    "    \"test\": df_tokenized_test_eval[\"test\"],\n",
    "})\n",
    "# df_tokenized_split.save_to_disk(\"../data/df_tokenized_split\")\n",
    "df_tokenized_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec224ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T15:08:35.091656Z",
     "start_time": "2022-02-15T15:08:35.086936Z"
    }
   },
   "outputs": [],
   "source": [
    "df_tokenized_split[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87fedbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T17:49:04.888153Z",
     "start_time": "2022-02-18T17:49:04.814886Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df[\"train\"][\"labels\"][49])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
